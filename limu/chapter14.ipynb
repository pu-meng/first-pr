{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdbd2065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4]) torch.float32\n",
      "torch.Size([2, 3, 4]) torch.float32\n",
      "a= tensor([0.4339, 0.0766])\n",
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "from numpy import mask_indices\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "embed=nn.Embedding(num_embeddings=10,embedding_dim=4)\n",
    "#Embedding(10, 4)是一个10行4列的矩阵，随机生成，可以学习\n",
    "#embed(x)是根据x的内容，返回对应的行，比如0行，1行，2，行，必须是0到9的整数\n",
    "print(embed.weight.shape,embed.weight.dtype)\n",
    "x=torch.tensor([[1,2,3],[4,5,6]])\n",
    "y=embed(x)\n",
    "\n",
    "print(y.shape,y.dtype)\n",
    "class SigmoidBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,inputs,target,mask=None):\n",
    "\n",
    "        out=nn.functional.binary_cross_entropy_with_logits(inputs,target,weight=mask,reduction='none')\n",
    "        return out.mean(dim=1)\n",
    "\n",
    "loss=SigmoidBCELoss()\n",
    "inputs=torch.randn(2,3)\n",
    "target=torch.randn(2,3)\n",
    "mask=torch.tensor([[1,1,0],[0,1,1]])\n",
    "a=loss(inputs,target,mask)\n",
    "print(\"a=\",a)\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    a=1/(1+math.exp(-x))\n",
    "    return -math.log(a)\n",
    "print(sigmoid(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26cdc70",
   "metadata": {},
   "source": [
    "$w_i$表示第$i$个词，$v_i$表示词$w_i$作中心词的词向量，$u_i$表示$w_i$作上下文词的词向量。$q_{ij}=P(w_j|w_i)$表示$w_j$出现在$w_i$的上下文中的概率。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(w_j|w_i) &= \\frac{q_{ij}}{\\sum_{k=1}^{n}q_{ik}} \\\\\n",
    "&=\\frac{\\exp(u_j^T v_i)}{\\sum_{k=1}^{n}\\exp(u_k^T v_i)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "GloVe模型,通过最小化词向量之间的余弦距离来学习词向量。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J &= \\sum_{i=1}^{N}\\sum_{j=1}^{N}h\\left(x_{i j}\\right)\\left(u_{j}^{T} v_{i}+\\boldsymbol{b}_{i}+\\boldsymbol{c}_{j}-\\log \\left(x_{ij}\\right)\\right)^{2} \\\\\n",
    "\n",
    "\n",
    "h\\left(x\\right) &= (x/c)^\\alpha \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$x_{i j}$表示词$i$和词$j$的共现次数，$w_i$和$v_i$是词$i$作为中心词和上下文词的词向量，$\\boldsymbol{b}_i$和$\\boldsymbol{c}_j$是偏置项。\n",
    "\n",
    "$$\n",
    "exp(u_j^T v_i)\\approx\\alpha p_{ij},\\qquad p_{ij}=x_{ij}/x_{i},\n",
    "$$\n",
    "两边取对数，$u_j^T v_i\\approx\\log \\alpha + \\log x_{ij}-\\log x_{i}$\n",
    "\n",
    "我们使用附加的偏置项来拟合$-\\log \\alpha+\\log x_{i},$,\n",
    "\n",
    "$$\n",
    "u_j^T v_i+b_i+c_j\\approx \\log x_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "$fastText$通过\"拆分词为子词，子词向量求和，表示整个词\",\n",
    "\n",
    "字节对编码(BPE,Byte Pair Encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b6302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练得到的BPE合并规则:\n",
      "('l', 'o') -> 合并顺序: 0\n",
      "('lo', 'w') -> 合并顺序: 1\n",
      "('e', 'r') -> 合并顺序: 2\n",
      "('low', 'e') -> 合并顺序: 3\n",
      "('lowe', 's') -> 合并顺序: 4\n",
      "('lowes', 't') -> 合并顺序: 5\n",
      "('n', 'e') -> 合并顺序: 6\n",
      "('ne', 'w') -> 合并顺序: 7\n",
      "('new', 'er') -> 合并顺序: 8\n",
      "('w', 'i') -> 合并顺序: 9\n",
      "单词 'lowest' 的编码结果: ['lowest']\n",
      "单词 'wider' 的编码结果: ['wi', 'd', 'er']\n",
      "单词 'newer' 的编码结果: ['newer']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "class BPE:\n",
    "    def __init__(self, num_merges=10):\n",
    "        \"\"\"\n",
    "        初始化BPE对象.\n",
    "        参数:\n",
    "            num_merges: 需要合并的次数\n",
    "        \"\"\"\n",
    "        self.num_merges = num_merges  # 设置合并次数\n",
    "        self.bpe_codes = {}  # 保存合并规则, key为需要合并的符号对, value为合并顺序\n",
    "    \n",
    "    def get_stats(self, vocab):\n",
    "        \"\"\"\n",
    "        计算当前词汇中所有符号对的频次.\n",
    "        参数:\n",
    "            vocab: dict, key为空格分隔的符号序列, value为词频\n",
    "        返回:\n",
    "            stats: dict, key为符号对(tuple), value为出现频次\n",
    "        \"\"\"\n",
    "        stats = collections.defaultdict(int)  # 使用defaultdict初始化统计字典\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()  # 将单词按空格分割成符号列表\n",
    "            # 遍历所有相邻符号对，统计频次\n",
    "            for i in range(len(symbols) - 1):\n",
    "                stats[(symbols[i], symbols[i + 1])] += freq\n",
    "        return stats\n",
    "\n",
    "    def merge_vocab(self, pair, vocab):\n",
    "        \"\"\"\n",
    "        在词汇中将符号对pair合并.\n",
    "        参数:\n",
    "            pair: tuple, 要合并的符号对\n",
    "            vocab: dict, 当前词汇\n",
    "        返回:\n",
    "            new_vocab: dict, 更新后的词汇\n",
    "        \"\"\"\n",
    "        new_vocab = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        for word, freq in vocab.items():\n",
    "            # 用合并后的符号替换原有的相邻符号\n",
    "            new_word = word.replace(bigram, replacement)\n",
    "            new_vocab[new_word] = freq\n",
    "        return new_vocab\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        使用给定数据训练BPE模型.\n",
    "        参数:\n",
    "            data: list of str, 每个字符串代表一个单词或句子，这里按单词拆分\n",
    "        \"\"\"\n",
    "        # 初始化词汇, 将每个单词拆分为字符，并用空格链接\n",
    "        vocab = {}\n",
    "        for word in data:\n",
    "            tokens = ' '.join(list(word))\n",
    "            vocab[tokens] = vocab.get(tokens, 0) + 1\n",
    "\n",
    "        # 进行num_merges次的合并\n",
    "        for i in range(self.num_merges):\n",
    "            stats = self.get_stats(vocab)\n",
    "            if not stats:\n",
    "                break\n",
    "            # 找到最频繁的符号对\n",
    "            best_pair = max(stats, key=stats.get)\n",
    "            self.bpe_codes[best_pair] = i\n",
    "            vocab = self.merge_vocab(best_pair, vocab)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def encode(self, word):\n",
    "        \"\"\"\n",
    "        根据训练好的BPE规则将单词编码为子词.\n",
    "        参数:\n",
    "            word: str, 输入的单词\n",
    "        返回:\n",
    "            tokens: list of str, BPE分词结果\n",
    "        \"\"\"\n",
    "        tokens = list(word)\n",
    "        # 按照合并顺序应用所有的BPE规则\n",
    "        for pair, _ in sorted(self.bpe_codes.items(), key=lambda x: x[1]):\n",
    "            bigram = list(pair)\n",
    "            merged = ''.join(bigram)\n",
    "            i = 0\n",
    "            # 在tokens中查找连续匹配bigram的部分并进行合并\n",
    "            while i < len(tokens) - 1:\n",
    "                if tokens[i] == bigram[0] and tokens[i + 1] == bigram[1]:\n",
    "                    tokens[i] = merged\n",
    "                    del tokens[i + 1]\n",
    "                else:\n",
    "                    i += 1\n",
    "        return tokens\n",
    "\n",
    "# 定义训练与测试数据\n",
    "if __name__ == \"__main__\":\n",
    "    # 训练数据\n",
    "    train_data = [\"low\", \"lowest\", \"newer\", \"wider\"]\n",
    "\n",
    "    # 初始化BPE对象，设置合并次数\n",
    "    bpe = BPE(num_merges=10)\n",
    "    bpe.fit(train_data)\n",
    "\n",
    "    print(\"训练得到的BPE合并规则:\")\n",
    "    for pair, order in bpe.bpe_codes.items():\n",
    "        print(f\"{pair} -> 合并顺序: {order}\")\n",
    "\n",
    "    # 测试编码\n",
    "    test_words = [\"lowest\", \"wider\", \"newer\"]\n",
    "    for word in test_words:\n",
    "        encoded = bpe.encode(word)\n",
    "        print(f\"单词 '{word}' 的编码结果: {encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f1e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "filename = \"text8.zip\"\n",
    "data_dir = \"text8_data\"\n",
    "\n",
    "# Download dataset if not already downloaded\n",
    "if not os.path.exists(filename):\n",
    "    print(\"Downloading Text8 dataset...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Extract dataset to a designated directory if not already extracted\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    print(\"Dataset extracted to\", data_dir)\n",
    "else:\n",
    "    print(\"Dataset already extracted in\", data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
