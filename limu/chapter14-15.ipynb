{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd68c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "    \n",
    "    def forward(self, center, context, negatives):\n",
    "        # center: [batch_size]\n",
    "        # context: [batch_size] (正例)\n",
    "        # negatives: [batch_size, k] (负例)\n",
    "        center_emb = self.embed(center)  # [batch_size, embed_dim]\n",
    "        context_emb = self.embed(context)  # [batch_size, embed_dim]\n",
    "        negatives_emb = self.embed(negatives)  # [batch_size, k, embed_dim]\n",
    "        \n",
    "        # 正例得分\n",
    "        pos_score = torch.sum(center_emb * context_emb, dim=1)  # [batch_size]\n",
    "        pos_loss = -F.logsigmoid(pos_score).mean()\n",
    "        \n",
    "        # 负例得分\n",
    "        neg_score = torch.bmm(negatives_emb, center_emb.unsqueeze(2)).squeeze(2)  # [batch_size, k]\n",
    "        neg_loss = -F.logsigmoid(-neg_score).mean()\n",
    "        \n",
    "        return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecff5a3",
   "metadata": {},
   "source": [
    "# mypy: allow-untyped-defs\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F, init\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from .module import Module\n",
    "\n",
    "\n",
    "__all__ = [\"Embedding\", \"EmbeddingBag\"]\n",
    "\n",
    "\n",
    "class Embedding(Module):\n",
    "    r\"\"\"A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "\n",
    "    This module is often used to store word embeddings and retrieve them using indices.\n",
    "    The input to the module is a list of indices, and the output is the corresponding\n",
    "    word embeddings.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): size of the dictionary of embeddings\n",
    "        embedding_dim (int): the size of each embedding vector\n",
    "        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n",
    "                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n",
    "                                     i.e. it remains as a fixed \"pad\". For a newly constructed Embedding,\n",
    "                                     the embedding vector at :attr:`padding_idx` will default to all zeros,\n",
    "                                     but can be updated to another value to be used as the padding vector.\n",
    "        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n",
    "                                    is renormalized to have norm :attr:`max_norm`.\n",
    "        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n",
    "        scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n",
    "                                                the words in the mini-batch. Default ``False``.\n",
    "        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n",
    "                                 See Notes for more details regarding sparse gradients.\n",
    "\n",
    "    Attributes:\n",
    "        weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n",
    "                         initialized from :math:`\\mathcal{N}(0, 1)`\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
    "        - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n",
    "\n",
    "    .. note::\n",
    "        Keep in mind that only a limited number of optimizers support\n",
    "        sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n",
    "        :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n",
    "\n",
    "    .. note::\n",
    "        When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the\n",
    "        :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be\n",
    "        modified in-place, performing a differentiable operation on ``Embedding.weight`` before\n",
    "        calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when\n",
    "        :attr:`max_norm` is not ``None``. For example::\n",
    "\n",
    "            n, d, m = 3, 5, 7\n",
    "            embedding = nn.Embedding(n, d, max_norm=1.0)\n",
    "            W = torch.randn((m, d), requires_grad=True)\n",
    "            idx = torch.tensor([1, 2])\n",
    "            a = (\n",
    "                embedding.weight.clone() @ W.t()\n",
    "            )  # weight must be cloned for this to be differentiable\n",
    "            b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "            out = a.unsqueeze(0) + b.unsqueeze(1)\n",
    "            loss = out.sigmoid().prod()\n",
    "            loss.backward()\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> # an Embedding module containing 10 tensors of size 3\n",
    "        >>> embedding = nn.Embedding(10, 3)\n",
    "        >>> # a batch of 2 samples of 4 indices each\n",
    "        >>> input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "        >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
    "        >>> embedding(input)\n",
    "        tensor([[[-0.0251, -1.6902,  0.7172],\n",
    "                 [-0.6431,  0.0748,  0.6969],\n",
    "                 [ 1.4970,  1.3448, -0.9685],\n",
    "                 [-0.3677, -2.7265, -0.1685]],\n",
    "\n",
    "                [[ 1.4970,  1.3448, -0.9685],\n",
    "                 [ 0.4362, -0.4004,  0.9400],\n",
    "                 [-0.6431,  0.0748,  0.6969],\n",
    "                 [ 0.9124, -2.3616,  1.1151]]])\n",
    "\n",
    "\n",
    "        >>> # example with padding_idx\n",
    "        >>> embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "        >>> input = torch.LongTensor([[0, 2, 0, 5]])\n",
    "        >>> embedding(input)\n",
    "        tensor([[[ 0.0000,  0.0000,  0.0000],\n",
    "                 [ 0.1535, -2.0309,  0.9315],\n",
    "                 [ 0.0000,  0.0000,  0.0000],\n",
    "                 [-0.1655,  0.9897,  0.0635]]])\n",
    "\n",
    "        >>> # example of changing `pad` vector\n",
    "        >>> padding_idx = 0\n",
    "        >>> embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n",
    "        >>> embedding.weight\n",
    "        Parameter containing:\n",
    "        tensor([[ 0.0000,  0.0000,  0.0000],\n",
    "                [-0.7895, -0.7089, -0.0364],\n",
    "                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n",
    "        >>> with torch.no_grad():\n",
    "        ...     embedding.weight[padding_idx] = torch.ones(3)\n",
    "        >>> embedding.weight\n",
    "        Parameter containing:\n",
    "        tensor([[ 1.0000,  1.0000,  1.0000],\n",
    "                [-0.7895, -0.7089, -0.0364],\n",
    "                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = [\n",
    "        \"num_embeddings\",\n",
    "        \"embedding_dim\",\n",
    "        \"padding_idx\",\n",
    "        \"max_norm\",\n",
    "        \"norm_type\",\n",
    "        \"scale_grad_by_freq\",\n",
    "        \"sparse\",\n",
    "    ]\n",
    "\n",
    "    num_embeddings: int\n",
    "    embedding_dim: int\n",
    "    padding_idx: Optional[int]\n",
    "    max_norm: Optional[float]\n",
    "    norm_type: float\n",
    "    scale_grad_by_freq: bool\n",
    "    weight: Tensor\n",
    "    freeze: bool\n",
    "    sparse: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da33acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "a= [2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1]\n",
      "a= [[1, 1, 1, 1, -1, -1]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_centers_and_contexts(corpus,max_window_size):\n",
    "    centers,contexts=[],[]\n",
    "    for i in range(len(corpus)):\n",
    "        center=corpus[i]\n",
    "        start=max(0,i-max_window_size)\n",
    "        end=min(len(corpus),i+max_window_size+1)\n",
    "        context=corpus[start:end]\n",
    "        if i>=max_window_size and i<len(corpus)-max_window_size:\n",
    "            centers.append(center)\n",
    "            contexts.append(context)\n",
    "    return centers,contexts\n",
    "class RandomGenerator:\n",
    "    def __init__(self,sampling_weights):\n",
    "\n",
    "        \"\"\"\n",
    "        初始化方法\n",
    "        参数:\n",
    "            sampling_weights: 采样权重列表，用于后续的采样操作\n",
    "        \"\"\"\n",
    "        self.population=list(range(len(sampling_weights)))  # 初始化种群，为每个权重创建一个索引\n",
    "        self.sampling_weights=sampling_weights  # 设置采样权重属性\n",
    "        self.candidates=[]  # 初始化候选列表，用于存储采样候选\n",
    "        self.i=0  # 初始化计数器，可能用于跟踪采样进度\n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        执行随机采样操作的方法\n",
    "        返回:\n",
    "            当前索引和对应的采样个体\n",
    "        \"\"\"\n",
    "        # 检查当前索引是否超出候选列表长度\n",
    "        if self.i>=len(self.candidates):\n",
    "            # 如果超出，则从当前种群中根据采样权重重新选择候选个体\n",
    "            # random.choices函数会根据权重列表sampling_weights从population中抽取与population等长的样本\n",
    "            self.candidates=random.choices(self.population,self.sampling_weights,k=len(self.population))\n",
    "            # 重置索引为0\n",
    "            self.i=0\n",
    "            #self.candiates是返回3个\n",
    "\n",
    "        # 索引递增\n",
    "        self.i+=1\n",
    "        # 返回当前选中的候选个体\n",
    "        return self.candidates[self.i-1]\n",
    "generator=RandomGenerator([2,3,4])\n",
    "a=[generator.draw() for _ in range(20)]\n",
    "print(generator.draw())\n",
    "\n",
    "print(\"a=\",a)\n",
    "\n",
    "def get_negatives(all_contexts,vocab,counter,k):\n",
    "    sampling_weights=[counter[vocab.to_token(i)]**0.75 for i in range(len(vocab))]\n",
    "    #计算采样权重，.to_token(i)将索引i转换为对应的单词，counter[vocab.to_token(i)]获取该单词的词频，然后计算词频的0.75次方作为采样权重\n",
    "    all_negatives=[]\n",
    "    #counter存储的是单词的词频\n",
    "    #vocab存储的是单词的索引\n",
    "    #vocab.to_token(i)将索引i转换为对应的单词\n",
    "    #all_contexts存储的是所有上下文\n",
    "    generator=RandomGenerator(sampling_weights)#创建一个随机生成器，用于根据采样权重进行采样\n",
    "    for contexts in all_contexts:\n",
    "        negatives=[]#初始化一个空列表，用于存储负样本\n",
    "        while len(negatives)<len(contexts)*k:\n",
    "            neg=generator.draw()#从采样器中抽取一个负样本\n",
    "            if neg not in contexts:   #如果负样本不在上下文中，则将其添加到负样本列表中\n",
    "                negatives.append(neg)#all_contexts存储的是上下文的索引，negatives存储的是负样本的索引\n",
    "        all_negatives.append(negatives)#每一个negatives大小为len(contexts)*k,存储的是负样本的索引，\n",
    "    return all_negatives\n",
    "#小批量加载训练实例\n",
    "def batchify(data):\n",
    "    \"\"\"返回带有负采样的跳远模型的小批量样本\"\"\"\n",
    "    maxlen=max(len(c)+len(n) for _,c,n in data  )#计算上下文和负样本的最大长度\n",
    "    centers,context_negatives,masks,labels=[],[],[],[]\n",
    "    #初始化四个空列表，分别存储中心词、上下文和负样本、掩码和标签\n",
    "    for center,context,negatives in data:\n",
    "        #遍历数据集中的每个样本\n",
    "\n",
    "        cur_len=len(context)+len(negatives)#计算当前样本的长度，即上下文和负样本的总数\n",
    "        centers+=[center]#将中心词添加到中心词列表中\n",
    "        contexts_negatives+=[context+negatives]#将上下文和负样本添加到上下文和负样本列表中\n",
    "        masks+=[ [1]*len(cur_len)+[0]*(maxlen-cur_len)]#将掩码添加到掩码列表中\n",
    "        labels+=[ [1]*len(context)+[-1]*len(negatives)+[0]*(maxlen-cur_len)]#将标签添加到标签列表中,\n",
    "        #-1表示负样本,maxlen-cur_len表示负样本的数量,[1]*len(context)表示正样本的数量\n",
    "    return torch.LongTensor(centers),torch.LongTensor(contexts_negatives),torch.FloatTensor(masks),torch.FloatTensor(labels)\n",
    "\n",
    "\n",
    "a=[[1]*4+[-1]*2]\n",
    "print(\"a=\",a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "limu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
