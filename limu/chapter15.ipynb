{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ad51b8",
   "metadata": {},
   "source": [
    "**第15章   自然语言处理：应用**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12294569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 14., 20., 26., 32., 38.])\n",
      "tensor([ 2.,  8., 14., 20., 26., 32.])\n",
      "k tensor([1, 2])\n",
      "k tensor([3, 4])\n",
      "k tensor([-1, -3])\n"
     ]
    }
   ],
   "source": [
    "import email\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "def corr1d(x,k):\n",
    "    w=k.shape[0]\n",
    "    Y=torch.zeros((x.shape[0]-w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y[i]=(x[i:i+w]*k).sum()\n",
    "    return Y\n",
    "def corr1d_multi_in(x,k):\n",
    "    y=sum(corr1d(x,k) for x,k in zip(x,k))\n",
    "    \n",
    "    \n",
    "    return y\n",
    "x=torch.tensor([0,1,2,3,4,5,6,7])\n",
    "k=torch.tensor([1,2,3])\n",
    "print(corr1d(x,k))\n",
    "x=torch.tensor([[0,1,2,3,4,5,6],[1,2,3,4,5,6,7],[2,3,4,5,6,7,8]])\n",
    "k=torch.tensor([[1,2],[3,4],[-1,-3]])\n",
    "print(corr1d_multi_in(x,k))\n",
    "for x,k in zip(x,k):\n",
    "    print(\"k\",k)\n",
    "def mlp(num_inputs,num_hiddens,flatten):\n",
    "    net=[]\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_inputs,num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_hiddens,num_inputs))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    return nn.Sequential(*net)\n",
    "class Attend(nn.Module):\n",
    "    def __init__(self,num_inputs,num_hiddens,**kwargs):\n",
    "        super().__init__()\n",
    "        self.f=mlp(num_inputs,num_hiddens,flatten=False)\n",
    "    def forward(self,A,B):\n",
    "        f_A=self.f(A)\n",
    "        f_B=self.f(B)\n",
    "        e=torch.bmm(f_A,f_B.permute(0,2,1))\n",
    "        beta=torch.softmax(F.softmax(e,dim=-1),B)\n",
    "        alpha=torch.bmm(F.softmax(e.permute(0,2,1),dim=-1),A)\n",
    "        return alpha,beta\n",
    "class Compare(nn.Module):\n",
    "    def __init__(self,num_inputs,num_hiddens,**kwargs):\n",
    "        super().__init__()\n",
    "        self.g=mlp(num_inputs,num_hiddens,flatten=False)\n",
    "    def forward(self,A,B,beta,alpha):\n",
    "        V_A=self.g(torch.cat([A,beta],dim=2))\n",
    "        V_B=self.g(torch.cat([B,alpha],dim=2))\n",
    "\n",
    "        return V_A,V_B\n",
    "class Aggregate(nn.Module):\n",
    "    def __init__(self,num_inputs,num_hiddens,num_outputs,**kwargs):\n",
    "        super().__init__()\n",
    "        self.h=mlp(num_inputs,num_hiddens,flatten=True)\n",
    "        self.linear=nn.Linear(num_hiddens,num_outputs)\n",
    "    def forward(self,V_A,V_B):\n",
    "        V_A=V_A.sum(dim=1)\n",
    "        V_B=V_B.sum(dim=1)\n",
    "        Y_hat=self.linear(self.h(torch.cat([V_A,V_B],dim=1)))\n",
    "        return Y_hat\n",
    "\n",
    "class DecomposableAttention(nn.Module):\n",
    "    def __init__(self,vocab,embed_size,num_hiddens,num_inputs_attend=100,num_inputs_compare=200,\n",
    "                 num_inputs_agg=400,**kwargs):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(len(vocab),embed_size)\n",
    "        self.attend=Attend(num_inputs_attend,num_hiddens)\n",
    "        self.compare=Compare(num_inputs_compare,num_hiddens)\n",
    "\n",
    "        self.aggregate=Aggregate(num_inputs_agg,num_hiddens,num_outputs=3)\n",
    "    def forward(self,x):\n",
    "        premises,hypotheses=x\n",
    "        A=self.embedding(premises)\n",
    "        B=self.embedding(hypotheses)\n",
    "        beta,alpha=self.attend(A,B)\n",
    "        V_A,V_B=self.compare(A,B,beta,alpha)\n",
    "        Y_hat=self.aggregate(V_A,V_B)\n",
    "        return Y_hat\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b22e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    \n",
    "    def forward(self, text): # text: [sent len, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "\n",
    "        return self.fc(hidden.squeeze(0)) # [batch size, output_dim]\n",
    "# 15.1.1 词汇表\n",
    "# 15.1.2 词嵌入\n",
    "# 15.1.3 循环神经网络\n",
    "# 15.1.4 双向循环神经网络\n",
    "# 15.1.5 双向长短期记忆网络\n",
    "\n",
    "# 15.2 机器翻译\n",
    "# 15.2.1 翻译模型\n",
    "# 15.2.2 注意力机制\n",
    "# 15.2.3 Transformer\n",
    "# 15.2.4 Transformer的编码器和解码器\n",
    "# 15.2.5 Transformer的注意力机制\n",
    "# 15.2.6 Transformer的掩蔽多头注意力\n",
    "# 15.2.7 Transformer的编码器和解码器\n",
    "\n",
    "# 15.3 文本分类\n",
    "# 15.3.1 文本分类模型\n",
    "# 15.3.2 文本分类的损失函数\n",
    "# 15.3.3 文本分类的评估指标\n",
    "# 15.3.4 文本分类的模型训练\n",
    "# 15.3.5 文本分类的模型评估\n",
    "# 15.3.6 文本分类的模型预测\n",
    "# 15.3.7 文本分类的模型保存和加载\n",
    "# 15.3.8 文本分类的模型微调\n",
    "# 15.3.9 文本分类的模型部署\n",
    "# 15.4 文本生成\n",
    "# 15.4.1 文本生成模型\n",
    "\n",
    "# 15.4.2 文本生成模型的训练\n",
    "# 15.4.3 文本生成模型的评估\n",
    "# 15.4.4 文本生成模型的预测\n",
    "# 15.4.5 文本生成模型的保存和加载\n",
    "# 15.4.6 文本生成模型的微调\n",
    "# 15.4.7 文本生成模型的部署\n",
    "# 15.5 文本摘要\n",
    "# 15.5.1 文本摘要模型\n",
    "# 15.5.2 文本摘要模型的训练\n",
    "# 15.5.3 文本摘要模型的评估\n",
    "# 15.5.4 文本摘要模型的预测\n",
    "# 15.5.5 文本摘要模型的保存和加载\n",
    "\n",
    "# 15.5.6 文本摘要模型的微调\n",
    "# 15.5.7 文本摘要模型的部署\n",
    "# 15.6 文本情感分析\n",
    "# 15.6.1 文本情感分析模型\n",
    "# 15.6.2 文本情感分析模型的训练\n",
    "# 15.6.3 文本情感分析模型的评估\n",
    "# 15.6.4 文本情感分析模型的预测\n",
    "# 15.6.5 文本情感分析模型的保存和加载\n",
    "\n",
    "# 15.6.6 文本情感分析模型的微调\n",
    "# 15.6.7 文本情感分析模型的部署\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ffdd9",
   "metadata": {},
   "source": [
    "以下是针对你列出的 **NLP核心主题** 整理的 **笔记、可运行代码、数据集推荐**，所有代码基于PyTorch实现（与你已有BiRNN代码风格一致），结构按你的列表顺序排列，方便循序渐进学习：\n",
    "\n",
    "\n",
    "# 15.1 基础组件：词汇表、词嵌入、循环神经网络\n",
    "## 15.1.1 词汇表（Vocabulary）\n",
    "### 核心笔记\n",
    "- **作用**：将文本中的离散词（如“苹果”“喜欢”）映射为连续整数ID（如1→“苹果”，2→“喜欢”），是文本转向量的第一步。\n",
    "- **核心概念**：\n",
    "  - 词表大小（vocab_size）：包含的唯一词数量（通常过滤低频词，避免词表过大）；\n",
    "  - 特殊符号：`<PAD>`（填充，统一序列长度）、`<UNK>`（未登录词，处理词表外的词）、`<SOS>`/`<EOS>`（生成任务的开始/结束符）。\n",
    "\n",
    "### 代码：构建词汇表\n",
    "```python\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=1):\n",
    "        # 初始化特殊符号（优先级最高，ID从0开始）\n",
    "        self.special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "        self.min_freq = min_freq  # 过滤低频词的阈值\n",
    "        self.word2id = {}  # 词→ID映射\n",
    "        self.id2word = {}  # ID→词映射\n",
    "        self.vocab_size = 0  # 词表大小\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        \"\"\"\n",
    "        从文本列表构建词表\n",
    "        texts: list，每个元素是一个句子（已分词，如[\"我 喜欢 编程\", \"苹果 很好吃\"]）\n",
    "        \"\"\"\n",
    "        # 1. 统计词频\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # 2. 过滤低频词，加入词表\n",
    "        # 先添加特殊符号\n",
    "        for token in self.special_tokens:\n",
    "            self.word2id[token] = len(self.word2id)\n",
    "        # 再添加普通词（词频≥min_freq）\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_freq:\n",
    "                self.word2id[word] = len(self.word2id)\n",
    "        \n",
    "        # 3. 构建反向映射\n",
    "        self.id2word = {id: word for word, id in self.word2id.items()}\n",
    "        # 4. 更新词表大小\n",
    "        self.vocab_size = len(self.word2id)\n",
    "\n",
    "    def text_to_ids(self, text):\n",
    "        \"\"\"将单个句子（分词后）转为ID序列\"\"\"\n",
    "        words = text.split()\n",
    "        return [\n",
    "            self.word2id.get(word, self.word2id['<UNK>'])  # 未登录词用<UNK>的ID\n",
    "            for word in words\n",
    "        ]\n",
    "\n",
    "    def ids_to_text(self, ids):\n",
    "        \"\"\"将ID序列转回文本（跳过<PAD>）\"\"\"\n",
    "        words = []\n",
    "        for id in ids:\n",
    "            word = self.id2word[id]\n",
    "            if word == '<PAD>':\n",
    "                continue\n",
    "            words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "\n",
    "# 示例：使用\n",
    "if __name__ == \"__main__\":\n",
    "    # 样本文本（已分词）\n",
    "    texts = [\"我 喜欢 编程\", \"编程 很 有趣\", \"我 喜欢 苹果\", \"苹果 是 水果\"]\n",
    "    # 构建词表（过滤词频<1的词，这里所有词都保留）\n",
    "    vocab = Vocabulary(min_freq=1)\n",
    "    vocab.build_vocab(texts)\n",
    "    # 查看词表\n",
    "    print(\"词表大小：\", vocab.vocab_size)\n",
    "    print(\"词→ID：\", vocab.word2id)\n",
    "    # 文本转ID\n",
    "    text = \"我 喜欢 水果\"\n",
    "    ids = vocab.text_to_ids(text)\n",
    "    print(f\"文本'{text}'→ID：\", ids)\n",
    "    # ID转文本\n",
    "    print(f\"ID {ids}→文本：\", vocab.ids_to_text(ids))\n",
    "```\n",
    "\n",
    "### 数据集\n",
    "- 无需额外数据集，可基于任意文本（如你之前的text8、自己的句子）构建词表。\n",
    "\n",
    "\n",
    "## 15.1.2 词嵌入（Word Embedding）\n",
    "### 核心笔记\n",
    "- **作用**：将词汇表的整数ID映射为低维连续向量（如1→[0.2, 0.5, -0.1]），让向量蕴含语义（如“国王”-“男人”+“女人”≈“女王”）。\n",
    "- **两种方式**：\n",
    "  1. 随机初始化：嵌入层随模型一起训练（适合特定任务）；\n",
    "  2. 预训练嵌入：加载Word2Vec/GloVe等预训练向量（适合数据量小时迁移学习）。\n",
    "\n",
    "### 代码1：随机初始化嵌入层（与模型训练）\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. 先构建词表（复用上面的Vocabulary类）\n",
    "texts = [\"我 喜欢 编程\", \"编程 很 有趣\", \"我 喜欢 苹果\", \"苹果 是 水果\"]\n",
    "vocab = Vocabulary(min_freq=1)\n",
    "vocab.build_vocab(texts)\n",
    "\n",
    "# 2. 定义嵌入层（随机初始化）\n",
    "embedding_dim = 10  # 嵌入向量维度\n",
    "embedding = nn.Embedding(\n",
    "    num_embeddings=vocab.vocab_size,  # 词表大小\n",
    "    embedding_dim=embedding_dim,      # 嵌入维度\n",
    "    padding_idx=vocab.word2id['<PAD>']# <PAD>的ID，嵌入向量固定为0\n",
    ")\n",
    "\n",
    "# 3. 示例：ID序列→嵌入向量\n",
    "text = \"我 喜欢 编程\"\n",
    "ids = torch.tensor(vocab.text_to_ids(text)).unsqueeze(0)  # [batch_size=1, seq_len=3]\n",
    "embedded = embedding(ids)  # [1, 3, 10]（批次大小×序列长度×嵌入维度）\n",
    "print(\"嵌入向量形状：\", embedded.shape)\n",
    "print(\"'<PAD>'的嵌入向量：\", embedding(torch.tensor([vocab.word2id['<PAD>']])))  # 全0\n",
    "```\n",
    "\n",
    "### 代码2：加载预训练GloVe嵌入\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def load_glove_embedding(glove_path, vocab, embedding_dim):\n",
    "    \"\"\"\n",
    "    加载GloVe预训练向量，构建嵌入层\n",
    "    glove_path: GloVe文件路径（如glove.6B.100d.txt）\n",
    "    vocab: 自定义Vocabulary对象\n",
    "    embedding_dim: 嵌入维度（需与GloVe一致，如100）\n",
    "    \"\"\"\n",
    "    # 1. 读取GloVe向量，存储为{词: 向量}\n",
    "    glove_dict = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vec = np.array(parts[1:], dtype=np.float32)\n",
    "            if len(vec) == embedding_dim:\n",
    "                glove_dict[word] = vec\n",
    "\n",
    "    # 2. 初始化嵌入矩阵\n",
    "    embedding_matrix = torch.randn(vocab.vocab_size, embedding_dim)  # 随机初始化未匹配的词\n",
    "    embedding_matrix[vocab.word2id['<PAD>']] = torch.zeros(embedding_dim)  # <PAD>设为0\n",
    "\n",
    "    # 3. 填充匹配的预训练向量\n",
    "    matched_count = 0\n",
    "    for word, id in vocab.word2id.items():\n",
    "        if word in glove_dict:\n",
    "            embedding_matrix[id] = torch.tensor(glove_dict[word])\n",
    "            matched_count += 1\n",
    "\n",
    "    print(f\"匹配到{matched_count}/{vocab.vocab_size}个词的预训练向量\")\n",
    "\n",
    "    # 4. 构建嵌入层（冻结或微调）\n",
    "    embedding = nn.Embedding(\n",
    "        num_embeddings=vocab.vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        padding_idx=vocab.word2id['<PAD>']\n",
    "    )\n",
    "    embedding.weight.data.copy_(embedding_matrix)\n",
    "    # 可选：冻结嵌入层（不训练，适合数据少）\n",
    "    # embedding.weight.requires_grad = False\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# 示例：使用（需先下载GloVe）\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 构建词表\n",
    "    texts = [\"i love programming\", \"programming is fun\", \"i love apple\", \"apple is fruit\"]\n",
    "    vocab = Vocabulary(min_freq=1)\n",
    "    vocab.build_vocab(texts)\n",
    "\n",
    "    # 2. 加载GloVe（需自行下载：https://nlp.stanford.edu/projects/glove/）\n",
    "    glove_path = \"glove.6B.100d.txt\"  # 100维GloVe\n",
    "    embedding_dim = 100\n",
    "    embedding = load_glove_embedding(glove_path, vocab, embedding_dim)\n",
    "\n",
    "    # 3. 测试\n",
    "    text = \"i love programming\"\n",
    "    ids = torch.tensor(vocab.text_to_ids(text)).unsqueeze(0)\n",
    "    embedded = embedding(ids)\n",
    "    print(\"预训练嵌入向量形状：\", embedded.shape)\n",
    "```\n",
    "\n",
    "### 数据集（预训练嵌入）\n",
    "- GloVe：https://nlp.stanford.edu/projects/glove/（推荐`glove.6B.100d.txt`，适合英文任务）；\n",
    "- Word2Vec：https://code.google.com/archive/p/word2vec/（Google预训练英文/中文向量）；\n",
    "- 中文预训练嵌入：https://github.com/Embedding/Chinese-Word-Vectors（包含多种中文Word2Vec/GloVe）。\n",
    "\n",
    "\n",
    "## 15.1.3 循环神经网络（RNN）\n",
    "### 核心笔记\n",
    "- **作用**：处理序列数据（如文本）时，通过“循环单元”保留历史信息（如读“我喜欢”时，记住“我”的信息，再处理“喜欢”）。\n",
    "- **局限**：梯度消失/爆炸（无法处理长序列，如超过50个词的句子），后续被LSTM/GRU替代。\n",
    "- **PyTorch接口**：`nn.RNN(input_size, hidden_size, num_layers, bidirectional)`，输入需为`[seq_len, batch_size, input_size]`（序列长度×批次大小×输入维度）。\n",
    "\n",
    "### 代码：基础RNN（序列分类示例）\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 数据准备（简单情感分类：正面=1，负面=0）\n",
    "texts = [\n",
    "    \"我 喜欢 编程\", \"编程 很 有趣\", \"苹果 很好吃\",  # 正面\n",
    "    \"我 讨厌 下雨\", \"作业 很难\", \"考试 不及格\"     # 负面\n",
    "]\n",
    "labels = torch.tensor([1, 1, 1, 0, 0, 0], dtype=torch.long)\n",
    "\n",
    "# 2. 构建词表和ID序列\n",
    "vocab = Vocabulary(min_freq=1)\n",
    "vocab.build_vocab(texts)\n",
    "# 文本转ID，并统一序列长度（用<PAD>填充到最长序列长度=3）\n",
    "seq_len = 3\n",
    "ids_list = []\n",
    "for text in texts:\n",
    "    ids = vocab.text_to_ids(text)\n",
    "    # 填充或截断到seq_len\n",
    "    if len(ids) < seq_len:\n",
    "        ids += [vocab.word2id['<PAD>']] * (seq_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:seq_len]\n",
    "    ids_list.append(ids)\n",
    "ids = torch.tensor(ids_list).permute(1, 0)  # 转置为[seq_len=3, batch_size=6]\n",
    "\n",
    "\n",
    "# 3. 定义RNN分类模型\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab.word2id['<PAD>'])\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim,  # 输入维度=嵌入维度\n",
    "            hidden_size=hidden_dim,    # 隐藏层维度\n",
    "            num_layers=1,              # 1层RNN\n",
    "            bidirectional=False        # 单向RNN\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # 分类头\n",
    "\n",
    "    def forward(self, text_ids):\n",
    "        # text_ids: [seq_len, batch_size]\n",
    "        embedded = self.embedding(text_ids)  # [seq_len, batch_size, embedding_dim]\n",
    "        output, hidden = self.rnn(embedded)  # output: [seq_len, batch_size, hidden_dim]; hidden: [1, batch_size, hidden_dim]\n",
    "        # 用最后一步的隐藏态做分类（RNN最后一步包含整个序列的信息）\n",
    "        return self.fc(hidden.squeeze(0))  # hidden.squeeze(0): [batch_size, hidden_dim]\n",
    "\n",
    "\n",
    "# 4. 训练模型\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 2  # 二分类（正面/负面）\n",
    "model = RNNClassifier(vocab.vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()  # 分类损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(ids)  # [batch_size=6, output_dim=2]\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 计算准确率\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = (preds == labels).float().mean()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Acc: {acc.item():.4f}\")\n",
    "\n",
    "# 5. 预测\n",
    "model.eval()\n",
    "test_text = \"我 喜欢 苹果\"\n",
    "test_ids = vocab.text_to_ids(test_text)\n",
    "test_ids = torch.tensor(test_ids).unsqueeze(1)  # [seq_len=3, batch_size=1]\n",
    "with torch.no_grad():\n",
    "    logits = model(test_ids)\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "print(f\"测试文本：{test_text}，预测情感：{'正面' if pred == 1 else '负面'}\")\n",
    "```\n",
    "\n",
    "### 数据集\n",
    "- 简单任务：可自定义文本（如上面的情感句子）；\n",
    "- 真实任务：IMDB情感数据集（英文，用于情感分类）、THUCNews（中文，用于文本分类）。\n",
    "\n",
    "\n",
    "## 15.1.4 双向循环神经网络（BiRNN）\n",
    "### 核心笔记\n",
    "- **作用**：解决普通RNN“只看过去，不看未来”的问题，通过两个方向的RNN（正向：从左到右；反向：从右到左）捕捉双向上下文。\n",
    "- **关键变化**：\n",
    "  - 隐藏层维度：双向RNN的输出维度=2×hidden_dim（正向+反向）；\n",
    "  - 隐藏态拼接：最后一步需将正向最后隐藏态和反向最后隐藏态拼接。\n",
    "\n",
    "### 代码：BiRNN分类（复用你之前的代码，补充训练）\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 数据准备（同15.1.3）\n",
    "texts = [\n",
    "    \"我 喜欢 编程\", \"编程 很 有趣\", \"苹果 很好吃\",\n",
    "    \"我 讨厌 下雨\", \"作业 很难\", \"考试 不及格\"\n",
    "]\n",
    "labels = torch.tensor([1, 1, 1, 0, 0, 0], dtype=torch.long)\n",
    "vocab = Vocabulary(min_freq=1)\n",
    "vocab.build_vocab(texts)\n",
    "seq_len = 3\n",
    "ids_list = []\n",
    "for text in texts:\n",
    "    ids = vocab.text_to_ids(text)\n",
    "    ids = ids + [vocab.word2id['<PAD>']]*(seq_len-len(ids)) if len(ids)<seq_len else ids[:seq_len]\n",
    "    ids_list.append(ids)\n",
    "ids = torch.tensor(ids_list).permute(1, 0)  # [seq_len=3, batch_size=6]\n",
    "\n",
    "\n",
    "# 2. 你的BiRNN代码（补充注释）\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(BiRNN, self).__init__()\n",
    "        # 词嵌入层\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, \n",
    "            padding_idx=vocab.word2id['<PAD>']  # 填充符嵌入为0\n",
    "        )\n",
    "        # 双向LSTM（用LSTM替代RNN，解决梯度消失）\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,  # 输入维度=嵌入维度\n",
    "            hidden_size=hidden_dim,    # 单向隐藏层维度\n",
    "            bidirectional=True         # 双向：True\n",
    "        )\n",
    "        # 分类头：输入维度=2×hidden_dim（正向+反向隐藏态）\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "    \n",
    "    def forward(self, text):  # text: [sent len, batch size]\n",
    "        # 1. 词嵌入：[sent len, batch size] → [sent len, batch size, embedding_dim]\n",
    "        embedded = self.embedding(text)\n",
    "        # 2. 双向LSTM：输出output=[sent len, batch size, 2×hidden_dim]；hidden=(正向隐藏态, 反向隐藏态)\n",
    "        # hidden形状：(num_layers×num_directions, batch size, hidden_dim) → 这里num_layers=1，num_directions=2\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # 3. 拼接正向最后隐藏态（hidden[-2,:,:]）和反向最后隐藏态（hidden[-1,:,:]）\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # [batch size, 2×hidden_dim]\n",
    "        # 4. 分类：[batch size, 2×hidden_dim] → [batch size, output_dim]\n",
    "        return self.fc(hidden)  # 原代码的squeeze(0)多余，因为hidden已无num_layers维度\n",
    "\n",
    "\n",
    "# 3. 训练与预测\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 2\n",
    "model = BiRNN(vocab.vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 训练\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(ids)  # [6, 2]\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = (preds == labels).float().mean()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Acc: {acc.item():.4f}\")\n",
    "\n",
    "# 预测\n",
    "model.eval()\n",
    "test_text = \"作业 很难\"\n",
    "test_ids = vocab.text_to_ids(test_text)\n",
    "test_ids = torch.tensor(test_ids).unsqueeze(1)  # [3, 1]\n",
    "with torch.no_grad():\n",
    "    logits = model(test_ids)\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "print(f\"测试文本：{test_text}，预测情感：{'正面' if pred == 1 else '负面'}\")\n",
    "```\n",
    "\n",
    "\n",
    "## 15.1.5 双向长短期记忆网络（BiLSTM）\n",
    "### 核心笔记\n",
    "- **作用**：在BiRNN基础上，用LSTM的“门控机制”（输入门、遗忘门、输出门）解决普通RNN的梯度消失问题，能处理更长序列。\n",
    "- **与BiRNN的关系**：BiLSTM是BiRNN的“升级版”（将RNN单元替换为LSTM单元），代码上仅需将`nn.RNN`改为`nn.LSTM`（如你之前的BiRNN代码已用LSTM）。\n",
    "- **关键结构**：\n",
    "  - 细胞状态（cell state）：类似“传送带”，可长期保存信息；\n",
    "  - 门控：控制信息的加入/遗忘/输出，避免梯度消失。\n",
    "\n",
    "### 代码：BiLSTM序列标注（命名实体识别示例）\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 数据准备（命名实体识别：O=非实体，PER=人名，LOC=地点）\n",
    "# 格式：(文本, 标签)，标签与词一一对应\n",
    "data = [\n",
    "    (\"小明 在 北京 工作\", [\"PER\", \"O\", \"LOC\", \"O\"]),\n",
    "    (\"小红 去 上海 旅游\", [\"PER\", \"O\", \"LOC\", \"O\"]),\n",
    "    (\"小李 在 广州 学习\", [\"PER\", \"O\", \"LOC\", \"O\"]),\n",
    "    (\"小张 住 在 深圳\", [\"PER\", \"O\", \"O\", \"LOC\"])\n",
    "]\n",
    "# 标签映射：O→0，PER→1，LOC→2\n",
    "label2id = {\"O\": 0, \"PER\": 1, \"LOC\": 2}\n",
    "id2label = {0: \"O\", 1: \"PER\", 2: \"LOC\"}\n",
    "\n",
    "# 2. 词表与ID转换\n",
    "texts = [item[0] for item in data]\n",
    "vocab = Vocabulary(min_freq=1)\n",
    "vocab.build_vocab(texts)\n",
    "seq_len = 4  # 最长序列长度\n",
    "# 文本转ID\n",
    "ids_list = []\n",
    "for text in texts:\n",
    "    ids = vocab.text_to_ids(text)\n",
    "    ids = ids + [vocab.word2id['<PAD>']]*(seq_len-len(ids)) if len(ids)<seq_len else ids[:seq_len]\n",
    "    ids_list.append(ids)\n",
    "ids = torch.tensor(ids_list).permute(1, 0)  # [seq_len=4, batch_size=4]\n",
    "# 标签转ID\n",
    "labels_list = []\n",
    "for _, labels in data:\n",
    "    label_ids = [label2id[label] for label in labels]\n",
    "    label_ids = label_ids + [0]*(seq_len-len(label_ids)) if len(label_ids)<seq_len else label_ids[:seq_len]\n",
    "    labels_list.append(label_ids)\n",
    "labels = torch.tensor(labels_list).permute(1, 0)  # [seq_len=4, batch_size=4]（序列标注需每个词对应一个标签）\n",
    "\n",
    "\n",
    "# 3. 定义BiLSTM序列标注模型\n",
    "class BiLSTM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab.word2id['<PAD>'])\n",
    "        self.bilstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, \n",
    "            bidirectional=True, batch_first=False\n",
    "        )\n",
    "        # 序列标注：每个词输出一个分类结果，输入维度=2×hidden_dim\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, text_ids):\n",
    "        # text_ids: [seq_len, batch_size]\n",
    "        embedded = self.embedding(text_ids)  # [seq_len, batch_size, embedding_dim]\n",
    "        output, (hidden, cell) = self.bilstm(embedded)  # output: [seq_len, batch_size, 2×hidden_dim]\n",
    "        logits = self.fc(output)  # [seq_len, batch_size, num_classes]（每个词的分类 logit）\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 4. 训练与预测\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "num_classes = 3  # O, PER, LOC\n",
    "model = BiLSTM_NER(vocab.vocab_size, embedding_dim, hidden_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略<PAD>对应的标签（这里标签0是O，实际应单独设<PAD>标签，简化用O）\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 训练\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(ids)  # [4, 4, 3]\n",
    "    # 交叉熵损失要求输入为[seq_len×batch_size, num_classes]，标签为[seq_len×batch_size]\n",
    "    loss = criterion(logits.reshape(-1, num_classes), labels.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 计算准确率（忽略<PAD>，这里简化不忽略）\n",
    "    preds = torch.argmax(logits, dim=2)  # [4, 4]\n",
    "    acc = (preds == labels).float().mean()\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Acc: {acc.item():.4f}\")\n",
    "\n",
    "# 预测\n",
    "model.eval()\n",
    "test_text = \"小刚 在 杭州 出差\"\n",
    "test_ids = vocab.text_to_ids(test_text)\n",
    "test_ids = torch.tensor(test_ids).unsqueeze(1)  # [4, 1]\n",
    "with torch.no_grad():\n",
    "    logits = model(test_ids)  # [4, 1, 3]\n",
    "    preds = torch.argmax(logits, dim=2).squeeze(1).tolist()  # [4]\n",
    "    pred_labels = [id2label[id] for id in preds]\n",
    "print(f\"测试文本：{test_text}\")\n",
    "print(f\"预测标签：{pred_labels}\")  # 期望：[\"PER\", \"O\", \"LOC\", \"O\"]\n",
    "```\n",
    "\n",
    "### 数据集\n",
    "- 命名实体识别：CoNLL2003（英文）、MSRA（中文）；\n",
    "- 序列标注通用：HuggingFace `datasets`库可直接加载（`from datasets import load_dataset`）。\n",
    "\n",
    "\n",
    "# 15.2 机器翻译\n",
    "## 15.2.1 翻译模型（基础Seq2Seq）\n",
    "### 核心笔记\n",
    "- **作用**：将一种语言（源语言，如中文）的序列转为另一种语言（目标语言，如英文）的序列。\n",
    "- **基础架构**：Seq2Seq（Encoder-Decoder）：\n",
    "  - Encoder：将源语言序列编码为固定长度的“上下文向量”；\n",
    "  - Decoder：基于上下文向量，逐词生成目标语言序列（自回归生成）。\n",
    "\n",
    "### 代码：基础Seq2Seq翻译（中→英，简化版）\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# 1. 数据准备（简单中译英）\n",
    "# (中文文本, 英文文本)\n",
    "data = [\n",
    "    (\"我 喜欢 编程\", \"i love programming\"),\n",
    "    (\"苹果 很好吃\", \"apple is delicious\"),\n",
    "    (\"编程 很 有趣\", \"programming is fun\"),\n",
    "    (\"我 讨厌 下雨\", \"i hate rain\")\n",
    "]\n",
    "\n",
    "# 2. 构建源语言（中文）和目标语言（英文）词表\n",
    "# 中文词表\n",
    "src_texts = [item[0] for item in data]\n",
    "src_vocab = Vocabulary(min_freq=1)\n",
    "src_vocab.build_vocab(src_texts)\n",
    "# 英文词表（需包含<SOS>/<EOS>）\n",
    "tgt_texts = [item[1] for item in data]\n",
    "tgt_vocab = Vocabulary(min_freq=1)\n",
    "tgt_vocab.build_vocab(tgt_texts)\n",
    "\n",
    "# 3. 数据预处理（源语言ID、目标语言输入ID、目标语言标签ID）\n",
    "seq_len = 3\n",
    "src_ids_list = []  # 源语言ID：[batch_size, seq_len]\n",
    "tgt_input_ids_list = []  # 目标语言输入（加<SOS>）：[batch_size, seq_len+1]\n",
    "tgt_label_ids_list = []  # 目标语言标签（加<EOS>）：[batch_size, seq_len+1]\n",
    "\n",
    "for src_text, tgt_text in data:\n",
    "    # 源语言ID\n",
    "    src_ids = src_vocab.text_to_ids(src_text)\n",
    "    src_ids = src_ids + [src_vocab.word2id['<PAD>']]*(seq_len-len(src_ids)) if len(src_ids)<seq_len else src_ids[:seq_len]\n",
    "    src_ids_list.append(src_ids)\n",
    "    \n",
    "    # 目标语言输入（开头加<SOS>）\n",
    "    tgt_ids = tgt_vocab.text_to_ids(tgt_text)\n",
    "    tgt_input_ids = [tgt_vocab.word2id['<SOS>']] + tgt_ids\n",
    "    tgt_input_ids = tgt_input_ids + [tgt_vocab.word2id['<PAD>']]*(seq_len+1-len(tgt_input_ids)) if len(tgt_input_ids)<seq_len+1 else tgt_input_ids[:seq_len+1]\n",
    "    tgt_input_ids_list.append(tgt_input_ids)\n",
    "    \n",
    "    # 目标语言标签（结尾加<EOS>）\n",
    "    tgt_label_ids = tgt_ids + [tgt_vocab.word2id['<EOS>']]\n",
    "    tgt_label_ids = tgt_label_ids + [tgt_vocab.word2id['<PAD>']]*(seq_len+1-len(tgt_label_ids)) if len(tgt_label_ids)<seq_len+1 else tgt_label_ids[:seq_len+1]\n",
    "    tgt_label_ids_list.append(tgt_label_ids)\n",
    "\n",
    "# 转为张量并调整维度（Seq2Seq输入需[seq_len, batch_size]）\n",
    "src_ids = torch.tensor(src_ids_list).permute(1, 0)  # [3, 4]\n",
    "tgt_input_ids = torch.tensor(tgt_input_ids_list).permute(1, 0)  # [4, 4]（seq_len+1=4）\n",
    "tgt_label_ids = torch.tensor(tgt_label_ids_list).permute(1, 0)  # [4, 4]\n",
    "\n",
    "\n",
    "# 4. 定义Seq2Seq模型（Encoder+Decoder）\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=src_vocab.word2id['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=False)\n",
    "    \n",
    "    def forward(self, src_ids):\n",
    "        # src_ids: [src_seq_len, batch_size]\n",
    "        embedded = self.embedding(src_ids)  # [src_seq_len, batch_size, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)  # hidden: [1, batch_size, hidden_dim]\n",
    "        # Encoder输出上下文向量：hidden（最后一步隐藏态）\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(tgt_vocab_size, embedding_dim, padding_idx=tgt_vocab.word2id['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_dim, tgt_vocab_size)  # 预测下一个词\n",
    "    \n",
    "    def forward(self, tgt_input_id, hidden, cell):\n",
    "        # tgt_input_id：当前步输入词的ID，形状[1, batch_size]（每次输入一个词）\n",
    "        embedded = self.embedding(tgt_input_id)  # [1, batch_size, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # output: [1, batch_size, hidden_dim]\n",
    "        logits = self.fc(output.squeeze(0))  # [batch_size, tgt_vocab_size]\n",
    "        return logits, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src_ids, tgt_input_ids):\n",
    "        # 1. Encoder编码源语言\n",
    "        hidden, cell = self.encoder(src_ids)\n",
    "        # 2. Decoder逐词生成目标语言\n",
    "        tgt_seq_len = tgt_input_ids.shape[0]  # 目标序列长度（含<SOS>）\n",
    "        batch_size = src_ids.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc.out_features\n",
    "        # 存储每一步的预测logits\n",
    "        outputs = torch.zeros(tgt_seq_len, batch_size, tgt_vocab_size)\n",
    "        \n",
    "        # 第一步输入：<SOS>（tgt_input_ids[0]）\n",
    "        input_id = tgt_input_ids[0].unsqueeze(0)  # [1, batch_size]\n",
    "        for t in range(tgt_seq_len):\n",
    "            logits, hidden, cell = self.decoder(input_id, hidden, cell)\n",
    "            outputs[t] = logits\n",
    "            # 下一步输入：当前步预测的词（贪心选择）\n",
    "            input_id = torch.argmax(logits, dim=1).unsqueeze(0)  # [1, batch_size]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# 5. 训练模型\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "# 初始化模型\n",
    "encoder = Encoder(src_vocab.vocab_size, embedding_dim, hidden_dim)\n",
    "decoder = Decoder(tgt_vocab.vocab_size, embedding_dim, hidden_dim)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "# 损失与优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.word2id['<PAD>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 训练\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(src_ids, tgt_input_ids)  # [4, 4, tgt_vocab_size]\n",
    "    # 损失计算：outputs→[tgt_seq_len×batch_size, tgt_vocab_size]；labels→[tgt_seq_len×batch_size]\n",
    "    loss = criterion(outputs.reshape(-1, tgt_vocab_size), tgt_label_ids.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 计算准确率\n",
    "    preds = torch.argmax(outputs, dim=2)  # [4, 4]\n",
    "    acc = (preds == tgt_label_ids).float().mean()\n",
    "    if (epoch + 1) % 40 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Acc: {acc.item():.4f}\")\n",
    "\n",
    "\n",
    "# 6. 预测（翻译）\n",
    "def translate(model, src_text, src_vocab, tgt_vocab, max_len=10):\n",
    "    model.eval()\n",
    "    # 源文本转ID\n",
    "    src_ids = src_vocab.text_to_ids(src_text)\n",
    "    src_ids = torch.tensor(src_ids).unsqueeze(1)  # [src_seq_len, 1]（batch_size=1）\n",
    "    # Encoder编码\n",
    "    hidden, cell = model.encoder(src_ids)\n",
    "    # Decoder生成\n",
    "    translated_ids = []\n",
    "    input_id = torch.tensor([[tgt_vocab.word2id['<SOS>']]])  # [1, 1]（<SOS>）\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        logits, hidden, cell = model.decoder(input_id, hidden, cell)\n",
    "        pred_id = torch.argmax(logits, dim=1).item()  # 贪心选择\n",
    "        # 若生成<EOS>，停止\n",
    "        if pred_id == tgt_vocab.word2id['<EOS>']:\n",
    "            break\n",
    "        translated_ids.append(pred_id)\n",
    "        # 下一步输入\n",
    "        input_id = torch.tensor([[pred_id]])\n",
    "    \n",
    "    # ID转文本\n",
    "    translated_text = tgt_vocab.ids_to_text(translated_ids)\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "# 测试翻译\n",
    "test_src_text = \"我 喜欢 苹果\"  # 未在训练集中\n",
    "translated_text = translate(model, test_src_text, src_vocab, tgt_vocab)\n",
    "print(f\"中文：{test_src_text} → 英文：{translated_text}\")  # 期望接近\"i love apple\"（视训练效果）\n",
    "```\n",
    "\n",
    "\n",
    "## 15.2.2 注意力机制（Attention）\n",
    "### 核心笔记\n",
    "- **问题**：基础Seq2Seq的Encoder输出固定长度上下文向量，无法处理长序列（信息丢失）。\n",
    "- **作用**：Decoder生成每个词时，动态“关注”Encoder中与当前词相关的源语言词（如翻译“programming”时，关注源语言的“编程”）。\n",
    "- **核心公式**：  \n",
    "  注意力权重 \\( \\alpha_{t,i} = \\frac{\\exp(\\text{score}(h_t, s_i))}{\\sum_j \\exp(\\text{score}(h_t, s_j))} \\)（\\( h_t \\)是Decoder当前隐藏态，\\( s_i \\)是Encoder第i步输出）；  \n",
    "  上下文向量 \\( c_t = \\sum_i \\alpha_{t,i} s_i \\)（加权求和Encoder输出）。\n",
    "\n",
    "### 代码：带注意力的Seq2Seq\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. 复用15.2.1的数据和词表（src_vocab, tgt_vocab, src_ids, tgt_input_ids, tgt_label_ids）\n",
    "\n",
    "# 2. 定义带注意力的Decoder\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, embedding_dim, hidden_dim, encoder_output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(tgt_vocab_size, embedding_dim, padding_idx=tgt_vocab.word2id['<PAD>'])\n",
    "        # LSTM输入：嵌入维度 + 注意力上下文向量维度（=encoder_output_dim）\n",
    "        self.lstm = nn.LSTM(embedding_dim + encoder_output_dim, hidden_dim)\n",
    "        # 注意力分数计算：Decoder隐藏态（hidden_dim） + Encoder输出（encoder_output_dim）→ 1维分数\n",
    "        self.attention = nn.Linear(hidden_dim + encoder_output_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, tgt_input_id, hidden, cell, encoder_outputs):\n",
    "        # tgt_input_id: [1, batch_size]\n",
    "        # hidden: [1, batch_size, hidden_dim]\n",
    "        # encoder_outputs: [src_seq_len, batch_size, encoder_output_dim]\n",
    "        \n",
    "        # 1. 词嵌入\n",
    "        embedded = self.embedding(tgt_input_id)  # [1, batch_size, embedding_dim]\n",
    "        \n",
    "        # 2. 计算注意力权重（对每个batch的每个源语言词）\n",
    "        src_seq_len = encoder_outputs.shape[0]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        # 扩展Decoder隐藏态到[src_seq_len, batch_size, hidden_dim]（与Encoder输出对齐）\n",
    "        hidden_expanded = hidden.repeat(src_seq_len, 1, 1)  # [src_seq_len, batch_size, hidden_dim]\n",
    "        # 拼接隐藏态和Encoder输出：[src_seq_len, batch_size, hidden_dim + encoder_output_dim]\n",
    "        concat = torch.cat((hidden_expanded, encoder_outputs), dim=2)\n",
    "        # 计算注意力分数：[src_seq_len, batch_size, 1] → 压缩为[src_seq_len, batch_size]\n",
    "        scores = self.attention(concat).squeeze(2)\n",
    "        # 归一化权重：[src_seq_len, batch_size]（每行和为1）\n",
    "        attn_weights = F.softmax(scores, dim=0)\n",
    "        \n",
    "        # 3. 计算上下文向量（加权求和Encoder输出）\n",
    "        # attn_weights扩展为[src_seq_len, batch_size, 1]，与Encoder输出做元素乘\n",
    "        attn_weights_expanded = attn_weights.unsqueeze(2)  # [src_seq_len, batch_size, 1]\n",
    "        # 加权求和：[batch_size, encoder_output_dim]（对src_seq_len维度求和）\n",
    "        context_vec = torch.sum(attn_weights_expanded * encoder_outputs, dim=0).unsqueeze(0)  # [1, batch_size, encoder_output_dim]\n",
    "        \n",
    "        # 4. 拼接嵌入向量和上下文向量（作为LSTM输入）\n",
    "        lstm_input = torch.cat((embedded, context_vec), dim=2)  # [1, batch_size, embedding_dim + encoder_output_dim]\n",
    "        \n",
    "        # 5. LSTM和分类\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))  # [1, batch_size, hidden_dim]\n",
    "        logits = self.fc(output.squeeze(0))  # [batch_size, tgt_vocab_size]\n",
    "        \n",
    "        return logits, hidden, cell, attn_weights\n",
    "\n",
    "\n",
    "# 3. 定义带注意力的Seq2Seq\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src_ids, tgt_input_ids):\n",
    "        # 1. Encoder编码：输出encoder_outputs（所有步输出）和最后隐藏态/细胞态\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src_ids)  # encoder_outputs: [src_seq_len, batch_size, hidden_dim]\n",
    "        \n",
    "        # 2. Decoder生成\n",
    "        tgt_seq_len = tgt_input_ids.shape[0]\n",
    "        batch_size = src_ids.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(tgt_seq_len, batch_size, tgt_vocab_size)\n",
    "        attn_weights_list = []  # 存储注意力权重，用于可视化\n",
    "        \n",
    "        input_id = tgt_input_ids[0].unsqueeze(0)\n",
    "        for t in range(tgt_seq_len):\n",
    "            logits, hidden, cell, attn_weights = self.decoder(input_id, hidden, cell, encoder_outputs)\n",
    "            outputs[t] = logits\n",
    "            attn_weights_list.append(attn_weights)\n",
    "            input_id = torch.argmax(logits, dim=1).unsqueeze(0)\n",
    "        \n",
    "        return outputs, attn_weights_list\n",
    "\n",
    "\n",
    "# 4. 重新定义Encoder（输出所有步的输出，而非仅最后隐藏态）\n",
    "class AttentionEncoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=src_vocab.word2id['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=False)\n",
    "    \n",
    "    def forward(self, src_ids):\n",
    "        embedded = self.embedding(src_ids)  # [src_seq_len, batch_size, embedding_dim]\n",
    "        encoder_outputs, (hidden, cell) = self.lstm(embedded)  # encoder_outputs: [src_seq_len, batch_size, hidden_dim]\n",
    "        return encoder_outputs, hidden, cell  # 输出所有步的encoder_outputs\n",
    "\n",
    "\n",
    "# 5. 训练与预测（复用15.2.1的训练逻辑，仅替换模型）\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "encoder = AttentionEncoder(src_vocab.vocab_size, embedding_dim, hidden_dim)\n",
    "decoder = AttentionDecoder(tgt_vocab.vocab_size, embedding_dim, hidden_dim, hidden_dim)\n",
    "model = Seq2SeqWithAttention(encoder, decoder)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.word2id['<PAD>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 训练\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    outputs, _ = model(src_ids, tgt_input_ids)\n",
    "    loss = criterion(outputs.reshape(-1, tgt_vocab_size), tgt_label_ids.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    preds = torch.argmax(outputs, dim=2)\n",
    "    acc = (preds == tgt_label_ids).float().mean()\n",
    "    if (epoch + 1) % 40 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Acc: {acc.item():.4f}\")\n",
    "\n",
    "# 预测（修改translate函数，支持注意力）\n",
    "def translate_with_attention(model, src_text, src_vocab, tgt_vocab, max_len=10):\n",
    "    model.eval()\n",
    "    src_ids = src_vocab.text_to_ids(src_text)\n",
    "    src_ids = torch.tensor(src_ids).unsqueeze(1)  # [src_seq_len, 1]\n",
    "    encoder_outputs, hidden, cell = model.encoder(src_ids)\n",
    "    translated_ids = []\n",
    "    attn_weights_list = []\n",
    "    input_id = torch.tensor([[tgt_vocab.word2id['<SOS>']]])\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        logits, hidden, cell, attn_weights = model.decoder(input_id, hidden, cell, encoder_outputs)\n",
    "        pred_id = torch.argmax(logits, dim=1).item()\n",
    "        if pred_id == tgt_vocab.word2id['<EOS>']:\n",
    "            break\n",
    "        translated_ids.append(pred_id)\n",
    "        attn_weights_list.append(attn_weights.squeeze(1).detach().numpy())  # 存储注意力权重\n",
    "        input_id = torch.tensor([[pred_id]])\n",
    "    \n",
    "    translated_text = tgt_vocab.ids_to_text(translated_ids)\n",
    "    return translated_text, attn_weights_list\n",
    "\n",
    "# 测试\n",
    "test_src_text = \"我 喜欢 苹果\"\n",
    "translated_text, attn_weights = translate_with_attention(model, test_src_text, src_vocab, tgt_vocab)\n",
    "print(f\"中文：{test_src_text} → 英文：{translated_text}\")\n",
    "print(\"注意力权重（每行对应目标词，每列对应源词）：\")\n",
    "for i, (word, weights) in enumerate(zip(translated_text.split(), attn_weights)):\n",
    "    print(f\"目标词'{word}'的注意力权重：{weights.round(3)}\")  # 权重高的源词是关注重点\n",
    "```\n",
    "\n",
    "\n",
    "## 15.2.3~15.2.7 Transformer（编码器/解码器/注意力/掩蔽）\n",
    "### 核心笔记\n",
    "- **问题**：RNN/LSTM无法并行计算（序列依赖），Transformer用“自注意力”替代循环结构，实现并行，同时通过“多头注意力”捕捉多维度依赖。\n",
    "- **核心架构**：\n",
    "  - 编码器（N层）：多头自注意力 + 前馈网络；\n",
    "  - 解码器（N层）：掩码多头自注意力（防止偷看未来词） + 编码器-解码器注意力 + 前馈网络；\n",
    "  - 位置编码：弥补无循环结构的位置信息丢失。\n",
    "\n",
    "### 代码：简化版Transformer（机器翻译）\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# 1. 位置编码（正弦余弦编码）\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # 初始化位置编码矩阵：[max_len, d_model]\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n",
    "        # 频率公式：10000^(-2i/d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [d_model/2]\n",
    "        # 偶数位置用sin，奇数位置用cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 扩展为[1, max_len, d_model]，方便批次处理\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)  # 不参与训练的缓冲区\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]  # 加位置编码\n",
    "        return x\n",
    "\n",
    "\n",
    "# 2. 多头注意力（Multi-Head Attention）\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model必须能被n_heads整除\"\n",
    "        self.d_k = d_model // n_heads  # 每个头的维度\n",
    "        self.n_heads = n_heads\n",
    "        # Q/K/V线性变换层\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        # 输出线性变换层\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q/k/v: [batch_size, seq_len, d_model]\n",
    "        # mask: [batch_size, 1, seq_len]（用于掩盖PAD）或[batch_size, seq_len, seq_len]（用于掩码自注意力）\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # 1. 线性变换并分多头：[batch_size, seq_len, d_model] → [batch_size, n_heads, seq_len, d_k]\n",
    "        q = self.w_q(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2. 计算注意力分数：Q*K^T / sqrt(d_k) → [batch_size, n_heads, seq_len_q, seq_len_k]\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # 3. 应用掩码（将掩码位置设为-1e9，softmax后为0）\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 4. 计算注意力权重和上下文向量\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # [batch_size, n_heads, seq_len_q, seq_len_k]\n",
    "        context = torch.matmul(attn_weights, v)  # [batch_size, n_heads, seq_len_q, d_k]\n",
    "        \n",
    "        # 5. 拼接多头结果并线性变换：[batch_size, seq_len_q, d_model]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        output = self.w_o(context)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# 3. 前馈网络（Feed-Forward Network）\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        return self.fc2(self.relu(self.fc1(x)))  # [batch_size, seq_len, d_model]\n",
    "\n",
    "\n",
    "# 4. 编码器层（Encoder Layer）\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 1. 多头自注意力 + 残差连接 + 层归一化\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)  # 自注意力：Q=K=V\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        # 2. 前馈网络 + 残差连接 + 层归一化\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 5. 解码器层（Decoder Layer）\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_self_attn = MultiHeadAttention(d_model, n_heads)  # 掩码自注意力\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads)        # 编码器-解码器注意力\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        # 层归一化\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, tgt_mask, src_mask):\n",
    "        # 1. 掩码自注意力（防止偷看未来词）\n",
    "        masked_attn_output, _ = self.masked_self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(masked_attn_output))\n",
    "        # 2. 编码器-解码器注意力（关注Encoder输出）\n",
    "        cross_attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
    "        # 3. 前馈网络\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 6. 完整Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, n_layers=6, \n",
    "                 n_heads=8, d_ff=2048, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        # 嵌入层\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model, padding_idx=src_vocab.word2id['<PAD>'])\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model, padding_idx=tgt_vocab.word2id['<PAD>'])\n",
    "        # 位置编码\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        # 编码器和解码器\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "        # 输出层（预测目标词）\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 模型参数初始化\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def make_src_mask(self, src_ids):\n",
    "        # 源语言掩码：掩盖<PAD>，形状[batch_size, 1, src_seq_len]\n",
    "        src_mask = (src_ids != src_vocab.word2id['<PAD>']).unsqueeze(1)  # [batch_size, 1, src_seq_len]\n",
    "        return src_mask\n",
    "\n",
    "    def make_tgt_mask(self, tgt_ids):\n",
    "        # 目标语言掩码：1. 掩盖<PAD>；2. 掩盖未来词（下三角矩阵）\n",
    "        batch_size = tgt_ids.size(0)\n",
    "        tgt_seq_len = tgt_ids.size(1)\n",
    "        # 未来词掩码：[batch_size, tgt_seq_len, tgt_seq_len]，下三角为1，上三角为0\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, tgt_seq_len, tgt_seq_len, device=tgt_ids.device), diagonal=1)).bool()\n",
    "        # PAD掩码：[batch_size, 1, tgt_seq_len]\n",
    "        pad_mask = (tgt_ids != tgt_vocab.word2id['<PAD>']).unsqueeze(1)\n",
    "        # 合并掩码：两者都为1才保留\n",
    "        tgt_mask = pad_mask & nopeak_mask  # [batch_size, tgt_seq_len, tgt_seq_len]\n",
    "        return tgt_mask\n",
    "\n",
    "    def forward(self, src_ids, tgt_ids):\n",
    "        # src_ids: [batch_size, src_seq_len]\n",
    "        # tgt_ids: [batch_size, tgt_seq_len]（目标语言输入，不含<EOS>）\n",
    "        \n",
    "        # 1. 嵌入 + 位置编码\n",
    "        src_emb = self.dropout(self.pos_enc(self.src_emb(src_ids)))  # [batch_size, src_seq_len, d_model]\n",
    "        tgt_emb = self.dropout(self.pos_enc(self.tgt_emb(tgt_ids)))  # [batch_size, tgt_seq_len, d_model]\n",
    "        \n",
    "        # 2. 生成掩码\n",
    "        src_mask = self.make_src_mask(src_ids)\n",
    "        tgt_mask = self.make_tgt_mask(tgt_ids)\n",
    "        \n",
    "        # 3. 编码器\n",
    "        enc_output = src_emb\n",
    "        for enc_layer in self.encoder:\n",
    "            enc_output = enc_layer(enc_output, src_mask)  # [batch_size, src_seq_len, d_model]\n",
    "        \n",
    "        # 4. 解码器\n",
    "        dec_output = tgt_emb\n",
    "        for dec_layer in self.decoder:\n",
    "            dec_output = dec_layer(dec_output, enc_output, tgt_mask, src_mask)  # [batch_size, tgt_seq_len, d_model]\n",
    "        \n",
    "        # 5. 输出层（预测下一个词）\n",
    "        logits = self.fc_out(dec_output)  # [batch_size, tgt_seq_len, tgt_vocab_size]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# 7. 训练与预测（复用15.2.1的数据，调整维度为[batch_size, seq_len]）\n",
    "# 重新处理数据（将之前的[seq_len, batch_size]转为[batch_size, seq_len]）\n",
    "src_ids = torch.tensor(src_ids_list)  # [4, 3]（batch_size=4, src_seq_len=3）\n",
    "tgt_input_ids = torch.tensor(tgt_input_ids_list)  # [4, 4]（batch_size=4, tgt_seq_len=4）\n",
    "tgt_label_ids = torch.tensor(tgt_label_ids_list)  # [4, 4]\n",
    "\n",
    "# 初始化模型（简化参数：d_model=16, n_layers=2, n_heads=2）\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab.vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab.vocab_size,\n",
    "    d_model=16,\n",
    "    n_layers=2,\n",
    "    n_heads=2,\n",
    "    d_ff=64,\n",
    "    dropout=0.1\n",
    ")\n",
    "# 初始化参数（Transformer需特殊初始化）\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# 损失与优化器\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.word2id['<PAD>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# 训练\n",
    "model.train()\n",
    "for epoch in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(src_ids, tgt_input_ids[:, :-1])  # tgt_input_ids[:, :-1]：去掉最后一列（避免与标签错位）\n",
    "    # 标签：tgt_label_ids[:, 1:]（去掉<SOS>）\n",
    "    loss = criterion(logits.reshape(-1, tgt_vocab.vocab_size), tgt_label_ids[:, 1:].reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 准确率\n",
    "    preds = torch.argmax(logits, dim=2)\n",
    "    acc = (preds == tgt_label_ids[:, 1:]).float().mean()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Acc: {acc.item():.4f}\")\n",
    "\n",
    "# 预测\n",
    "def transformer_translate(model, src_text, src_vocab, tgt_vocab, max_len=10):\n",
    "    model.eval()\n",
    "    # 源文本转ID：[1, src_seq_len]（batch_size=1）\n",
    "    src_ids = src_vocab.text_to_ids(src_text)\n",
    "    src_ids = torch.tensor(src_ids).unsqueeze(0)\n",
    "    # 初始化目标序列：[1, 1]（<SOS>）\n",
    "    tgt_ids = torch.tensor([[tgt_vocab.word2id['<SOS>']]])\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # 预测\n",
    "        logits = model(src_ids, tgt_ids)  # [1, tgt_seq_len, tgt_vocab_size]\n",
    "        # 取最后一步的预测\n",
    "        pred_id = torch.argmax(logits[:, -1, :], dim=1).item()\n",
    "        # 若生成<EOS>，停止\n",
    "        if pred_id == tgt_vocab.word2id['<EOS>']:\n",
    "            break\n",
    "        # 追加到目标序列\n",
    "        tgt_ids = torch.cat((tgt_ids, torch.tensor([[pred_id]])), dim=1)\n",
    "    \n",
    "    # ID转文本（去掉<SOS>）\n",
    "    translated_ids = tgt_ids.squeeze(0)[1:].tolist()\n",
    "    translated_text = tgt_vocab.ids_to_text(translated_ids)\n",
    "    return translated_text\n",
    "\n",
    "# 测试\n",
    "test_src_text = \"我 喜欢 苹果\"\n",
    "translated_text = transformer_translate(model, test_src_text, src_vocab, tgt_vocab)\n",
    "print(f\"中文：{test_src_text} → 英文：{translated_text}\")\n",
    "```\n",
    "\n",
    "### 数据集（机器翻译）\n",
    "- 小规模：Multi30k（英德/英法，适合测试，HuggingFace可加载）；\n",
    "- 中规模：IWSLT2017（多语言，如中英，句子较短）；\n",
    "- 大规模：WMT2014（英法/英德，适合训练高性能模型）；\n",
    "- 中文相关：OPUS-100（包含中英，https://www.opus.nlpl.eu/opus-100.php）。\n",
    "\n",
    "\n",
    "# 15.3 文本分类（完整流程）\n",
    "## 15.3.1~15.3.9 文本分类全流程（以IMDB情感分类为例）\n",
    "### 核心笔记\n",
    "- **任务**：将文本（如电影评论）分为预定义类别（如正面/负面）。\n",
    "- **完整流程**：数据加载→预处理→模型构建→损失函数→训练→评估→预测→保存/加载→微调→部署。\n",
    "- **常用模型**：CNN、BiLSTM、BERT（微调）。\n",
    "\n",
    "### 代码：BERT微调文本分类（IMDB情感分类）\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 15.3.1 数据加载（IMDB情感分类：正面=1，负面=0）\n",
    "dataset = load_dataset(\"imdb\")  # 自动下载IMDB数据集\n",
    "print(\"数据集结构：\", dataset)  # train(25k) + test(25k)\n",
    "print(\"样本示例：\", dataset[\"train\"][0])\n",
    "\n",
    "# 15.3.2 数据预处理（Tokenizer）\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  # 加载BERT分词器\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # 分词：max_length=128（截断/填充）\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# 应用预处理\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "# 转换为PyTorch格式\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# 简化：用小数据集加速训练\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# 15.3.3 模型构建（BERT分类模型）\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2  # 二分类（正面/负面）\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "# 第15章 自然语言处理：应用\n",
    "\n",
    "本章讨论了自然语言处理的多个任务，例如：\n",
    "- 15.1 双向循环神经网络（BiRNN/双向LSTM）：用于模型编码\n",
    "- 15.2 机器翻译及注意力机制\n",
    "- 15.3 文本分类：包括模型定义、损失函数、评估、训练、预测、保存和加载\n",
    "- 15.4 文本生成：模型训练、评估和部署\n",
    "- 15.5 文本摘要：模型构建、训练、预测和微调\n",
    "- 15.6 文本情感分析：建立情感分析模型并进行训练和评估\n",
    "\n",
    "以下代码展示了文本分类的一个示例，使用前面定义的 BiRNN 模型进行前向传播，并构造了一个简单的语料库及词汇表。\n",
    "\"\"\"\n",
    "\n",
    "# 构造一个简单的语料库，及文本分类的标签（例如 1 为正面，0 为负面）\n",
    "texts = [\n",
    "    \"this is good\",\n",
    "    \"this is bad\",\n",
    "    \"i love this\",\n",
    "    \"i hate that\"\n",
    "]\n",
    "labels = [1, 0, 1, 0]\n",
    "\n",
    "# 15.1.1 词汇表：构建简单的单词到索引映射\n",
    "word2idx = {\"<pad>\": 0}  # 用 0 表示 padding\n",
    "for text in texts:\n",
    "    for word in text.lower().split():\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "# 打印词汇表\n",
    "print(\"Vocabulary:\", word2idx)\n",
    "\n",
    "# 将文本转换为索引列表\n",
    "tokenized_texts = []\n",
    "for text in texts:\n",
    "    tokens = [word2idx[word] for word in text.lower().split()]\n",
    "    tokenized_texts.append(tokens)\n",
    "\n",
    "# 为了批量处理，需要 padding 每个序列到相同长度\n",
    "max_len = max(len(t) for t in tokenized_texts)\n",
    "padded_texts = [t + [word2idx[\"<pad>\"]] * (max_len - len(t)) for t in tokenized_texts]\n",
    "\n",
    "\n",
    "# 转换成 tensor，并调整形状为 [句子长度, batch size]\n",
    "text_tensor = torch.tensor(padded_texts, dtype=torch.long).transpose(0, 1)\n",
    "label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "print(\"Text Tensor shape:\", text_tensor.shape)\n",
    "print(\"Label Tensor shape:\", label_tensor.shape)\n",
    "\n",
    "# 参数设置\n",
    "vocab_size = len(word2idx)\n",
    "embedding_dim = 10\n",
    "hidden_dim = 16\n",
    "output_dim = 2  # 假设有两个类别：正面和负面\n",
    "\n",
    "# 实例化之前定义的 BiRNN 模型 (已在前面的单元中定义)\n",
    "model = BiRNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# 前向传播，获得预测结果\n",
    "outputs = model(text_tensor)\n",
    "print(\"Model outputs:\", outputs)\n",
    "\n",
    "# （后续可继续完成训练过程、损失计算、以及模型评估等任务）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "limu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
