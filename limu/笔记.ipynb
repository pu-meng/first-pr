{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a812a1a1",
   "metadata": {},
   "source": [
    "哪些是 Python 的 “内置函数 / 方法”？\n",
    "\n",
    "内置函数 / 方法指的是 Python 解释器自带、无需导入即可使用的功能，主要分为两类：\n",
    "\n",
    "（1）全局内置函数（直接调用）:\n",
    "\n",
    "全局内置函数是 Python 解释器自带的函数，无需导入即可使用。\n",
    "\n",
    "基础操作：print()（打印）、len()（获取长度）、type()（查看类型）、isinstance()（判断类型）。\n",
    "\n",
    "类型转换：int()、str()、list()、dict()、set()（转换数据类型）。\n",
    "\n",
    "迭代相关：range()（生成范围）、sorted()（排序）、sum()（求和）、max()/min()（最大 / 最小值）。\n",
    "\n",
    "其他：abs()（绝对值）、enumerate()（枚举）、zip()（打包多个可迭代对象）等。\n",
    "\n",
    "（2）内置数据类型的方法（需通过对象调用）\n",
    "\n",
    "内置数据类型的方法是 Python 解释器自带的，但需要通过对象来调用。\n",
    "\n",
    "字符串（str）：split()、join()、strip()（去首尾空格）、replace()（替换）、upper()/lower()（大小写转换）等。\n",
    "\n",
    "列表（list）：append()（添加元素）、pop()（删除元素）、sort()（排序）、extend()（扩展列表）等。\n",
    "\n",
    "字典（dict）：get()、keys()（获取所有键）、values()（获取所有值）、items()（获取键值对）等。\n",
    "\n",
    "集合（set）：add()（添加元素）、union()（并集）、intersection()（交集）等。\n",
    "\n",
    "使用注意事项:\n",
    "\n",
    "内置功能无需导入：比如print()、str.split()可以直接用，而collections.defaultdict需要先import collections。\n",
    "\n",
    "\n",
    "方法与函数的区别：\n",
    "\n",
    "函数是独立的（如len(list)）；\n",
    "\n",
    "方法是依附于对象的（如list.append(1)，必须通过具体对象调用）。\n",
    "\n",
    "默认参数的坑：\n",
    "\n",
    "比如str.split()默认分割空白字符，可能和预期不符（如需按单个空格分割，需显式指定sep=' '）。\n",
    "\n",
    "dict.get()如果不指定default，默认返回None，可能导致后续运算错误（如代码中显式指定0更安全）。\n",
    "\n",
    "可迭代对象的要求：join()、sorted()等函数要求输入是可迭代对象（列表、元组、字符串等），否则会报错。\n",
    "\n",
    "标准库与内置的区别：collections、os、sys等属于标准库（需要import），但不属于 “内置”（内置指无需导入即可用）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859da83",
   "metadata": {},
   "source": [
    "**BERT双向编码表示**\n",
    "\n",
    "BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer的预训练语言模型，它通过双向编码表示来理解自然语言。BERT模型的主要特点包括：\n",
    "\n",
    "1. 双向编码：BERT模型使用Transformer的编码器部分，该部分由多个自注意力层堆叠而成。每个自注意力层都会同时考虑输入序列中的所有位置，从而实现双向编码。这意味着BERT模型可以理解词语之间的上下文关系，例如“我喜欢吃苹果”和“我不喜欢吃苹果”中的“苹果”在不同的上下文中具有不同的含义。\n",
    "2. 预训练和微调：BERT模型首先在大量文本数据上进行预训练，学习语言的基本特征和模式。然后，在特定任务上进行微调，以适应特定任务的特定需求。预训练和微调的结合使得BERT模型在各种自然语言处理任务上取得了显著的性能提升。\n",
    "3. 丰富的应用场景：BERT模型可以应用于各种自然语言处理任务，包括文本分类、命名实体识别、情感分析、问答系统等。此外，BERT模型还可以与其他模型结合使用，以实现更复杂的任务，如文本生成、机器翻译等。\n",
    "\n",
    "**BERT模型的工作原理**\n",
    "\n",
    "BERT模型的工作原理可以分为两个阶段：预训练和微调。\n",
    "\n",
    "1. 预训练阶段：在预训练阶段，BERT模型首先在大规模文本数据上进行预训练，学习语言的基本特征和模式。预训练任务主要包括两个任务：掩码语言模型（Masked Language Model，MLM）和下一句预测（Next Sentence Prediction，NSP）。MLM任务的目标是预测被随机掩码的词语，而NSP任务的目标是判断两个句子是否是连续的。通过预训练，BERT模型可以学习到词语的上下文表示和句子之间的关系。  \n",
    "2. 微调阶段：在微调阶段，BERT模型在特定任务上进行微调，以适应特定任务的特定需求。微调任务主要包括文本分类、命名实体识别、情感分析等。在微调过程中，BERT模型的参数会被调整，以适应特定任务的特定需求。微调后的BERT模型可以应用于各种自然语言处理任务，如文本分类、命名实体识别、情感分析等。\n",
    "\n",
    "BERT模型和Transformer模型的关系,\n",
    "BERT模型是基于Transformer模型的一种预训练语言模型。Transformer模型是一种基于自注意力机制的深度神经网络模型，它通过自注意力机制来处理输入序列中的每个位置，从而实现高效的并行计算。BERT模型在Transformer模型的基础上，引入了双向编码表示和预训练-微调的框架，从而在自然语言处理任务上取得了显著的性能提升。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "limu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
