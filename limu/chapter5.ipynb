{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f35a12ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1281, -0.1629,  0.2090, -0.0678, -0.5984, -0.1955,  0.3527,  0.0543,\n",
      "          0.0136, -0.2148],\n",
      "        [ 0.0316,  0.0547,  0.1158, -0.3020, -0.1892, -0.1744,  0.3216, -0.1953,\n",
      "          0.2379, -0.1289]], grad_fn=<AddmmBackward0>)\n",
      "y= tensor([[-0.1403, -0.0502, -0.2725, -0.1243,  0.3843, -0.1088,  0.0984,  0.0310,\n",
      "         -0.2574,  0.2286],\n",
      "        [-0.3636, -0.0896,  0.0646,  0.0351,  0.4914,  0.1473,  0.1491, -0.2169,\n",
      "          0.3327, -0.0761]], grad_fn=<AddmmBackward0>)\n",
      "FixedHiddenMLP= tensor(-0.0090, grad_fn=<SumBackward0>)\n",
      "chimera= tensor(0.1423, grad_fn=<SumBackward0>)\n",
      "rgnet= tensor([[0.3622],\n",
      "        [0.3622]], grad_fn=<AddmmBackward0>)\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "rgnet[0]= Sequential(\n",
      "  (block 0): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (block 1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (block 2): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (block 3): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n",
      "rgnet[0][1]= Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "rgnet[0][1][0]= Linear(in_features=4, out_features=8, bias=True)\n",
      "rgnet[0][1][0].weight= Parameter containing:\n",
      "tensor([[ 0.0108, -0.1130, -0.3113,  0.0180],\n",
      "        [-0.3654,  0.4430,  0.0595,  0.3311],\n",
      "        [ 0.4929,  0.2054,  0.4977, -0.3820],\n",
      "        [-0.2435,  0.4638, -0.1531, -0.3197],\n",
      "        [-0.3283, -0.3517, -0.0933, -0.1770],\n",
      "        [-0.0109, -0.3675,  0.1592,  0.1433],\n",
      "        [-0.2683, -0.4105,  0.2591,  0.2531],\n",
      "        [-0.0393, -0.0598,  0.3306, -0.1080]], requires_grad=True)\n",
      "tensor([ 0.3208, -0.2668, -0.4457, -0.2451,  0.3623,  0.1802, -0.1067, -0.4391])\n",
      "net[0].weight= tensor([-0.0184,  0.0053,  0.0201, -0.0070])\n",
      "net[0].weight= tensor([-0.5686,  0.1218, -0.2148, -0.5919])\n",
      "net[2].weight= tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n",
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-8.7140,  0.0000,  0.0000, -7.2266],\n",
       "        [ 7.0398,  5.1997, -0.0000,  0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "net=nn.Sequential(\n",
    "    nn.Linear(20,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,10)\n",
    ")\n",
    "x=torch.randn(2,20)\n",
    "y=net(x)\n",
    "print(y)\n",
    "#自定义块\n",
    "#不继承nn.Module,模型失去PyTorch的很多功能,比如参数管理、保存和加载等\n",
    "#没有__init_()方法,forward()方法，无法定义可学习的参数，连基本的网络骨架都无法搭建\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "    def forward(self,x):\n",
    "        y=self.hidden(x)\n",
    "        y=torch.relu(y)\n",
    "        y=self.out(y)\n",
    "        return y\n",
    "net=MLP()#实例化,类是模板,对象才是实体，类是设计图，对象是盖出来的房子\n",
    "#实例化在内存中分配独立空间，初始化专属属性，获得类的方法使用权\n",
    "y=net(x)\n",
    "print(\"y=\",y)\n",
    "#自定义顺序块\n",
    "#enumerate()函数,同时获取索引和值\n",
    "#self._modules属性,存储子模块的有序字典\n",
    "#self._modules.values()方法,按添加顺序返回所有子模块\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for idx,module in enumerate(args):\n",
    "            self._modules[str(idx)]=module\n",
    "            #self.add_module(str(idx),module)等价于上面一句\n",
    "    def forward(self,x):\n",
    "        for module in self._modules.values():\n",
    "            x=module(x)\n",
    "        return x\n",
    "\n",
    "net=MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(x)\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight=torch.rand((20,20),requires_grad=False)\n",
    "        self.linear=nn.Linear(20,20)\n",
    "    def forward(self,x):\n",
    "        x=self.linear(x)\n",
    "        x=torch.mm(x,self.rand_weight)+1\n",
    "        x=torch.relu(x)\n",
    "        #x=torch.relu(torch.mm(x,self.rand_weight)+1)\n",
    "        x=self.linear(x)\n",
    "        while x.abs().sum()>1:\n",
    "            x/=2\n",
    "        return x.sum()\n",
    "net=FixedHiddenMLP()\n",
    "print(\"FixedHiddenMLP=\",net(x) )\n",
    "#混合搭配\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(20,64),nn.ReLU(),nn.Linear(64,32),nn.ReLU())\n",
    "        self.linear=nn.Linear(32,16)\n",
    "    def forward(self,x):\n",
    "        x=self.net(x)\n",
    "        x=self.linear(x)\n",
    "        return x\n",
    "chimera=nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())\n",
    "print(\"chimera=\",chimera(x))\n",
    "#从嵌套块收集参数\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,4),nn.ReLU())\n",
    "def block2():\n",
    "    net=nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}',block1())\n",
    "    return net\n",
    "rgnet=nn.Sequential(block2(),nn.Linear(4,1))\n",
    "x=torch.randn(2,4)\n",
    "y=rgnet(x)\n",
    "print(\"rgnet=\",y)\n",
    "print(rgnet)\n",
    "print(\"rgnet[0]=\",rgnet[0])\n",
    "print(\"rgnet[0][1]=\",rgnet[0][1])\n",
    "print(\"rgnet[0][1][0]=\",rgnet[0][1][0])\n",
    "print(\"rgnet[0][1][0].weight=\",rgnet[0][1][0].weight)\n",
    "print(rgnet[0][1][0].bias.data)\n",
    "\n",
    "#初始化参数\n",
    "def init_normal(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "def init_constant(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "net.apply(init_normal)\n",
    "\n",
    "print(\"net[0].weight=\",net[0].weight.data[0])\n",
    "\n",
    "def init_xavier(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(\"net[0].weight=\",net[0].weight.data[0])\n",
    "print(\"net[2].weight=\",net[2].weight.data)\n",
    "def my_init(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        print(\"Init\",*[(name,param.shape) for name,param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight,-10,10)\n",
    "        m.weight.data *=m.weight.data.abs()>=5\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b8e59",
   "metadata": {},
   "source": [
    "**5.2  参数管理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fdbbec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4812],\n",
      "        [-0.3488]], grad_fn=<AddmmBackward0>)\n",
      "OrderedDict([('weight', tensor([[ 0.2345,  0.3185, -0.2397, -0.2912, -0.2853, -0.3319,  0.0909,  0.1739]])), ('bias', tensor([-0.0305]))])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.0305], requires_grad=True)\n",
      "tensor([-0.0305])\n",
      "[('weight', torch.Size([8, 4])), ('bias', torch.Size([8]))]\n",
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "<generator object <genexpr> at 0x7a706aa735a0>\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "net=nn.Sequential(\n",
    "    nn.Linear(4,8),\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(8,1)\n",
    ")\n",
    "x=torch.randn(2,4)\n",
    "y=net(x)\n",
    "print(y)\n",
    "print(net[2].state_dict())#访问模型参数\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n",
    "net[2].weight.grad==None\n",
    "print([(name,param.shape) for name,param in net[0].named_parameters()])#访问所有模型参数\n",
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])#访问所有模型参数\n",
    "print((name,param.shape) for name,param in net.named_parameters())#访问所有模型参数\n",
    "\n",
    "#参数绑定\n",
    "shared=nn.Linear(8,8)\n",
    "net=nn.Sequential(\n",
    "    nn.Linear(4,8),\n",
    "    nn.ReLU(),\n",
    "    shared,\n",
    "    nn.ReLU(),\n",
    "    shared,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8,1)\n",
    ")\n",
    "print(net[2].weight.data[0]==net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ba5bc",
   "metadata": {},
   "source": [
    "**5.4  自定义层**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4164d73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2., -1.,  0.,  1.,  2.])\n",
      "Parameter containing:\n",
      "tensor([[-2.1974,  0.6721,  1.8472],\n",
      "        [ 0.6637,  0.7572, -1.7577],\n",
      "        [ 0.9021,  1.1366, -1.1014],\n",
      "        [-0.2501,  2.3138,  0.3873],\n",
      "        [-1.0540, -0.5400, -0.2885]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    " \n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x-x.mean()\n",
    "layer=CenteredLayer()\n",
    "print(layer(torch.FloatTensor([1,2,3,4,5])))\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self,in_units,out_units):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.weight=nn.Parameter(torch.randn(in_units,out_units))\n",
    "        self.bias=nn.Parameter(torch.randn(out_units,))\n",
    "    def forward(self,x):\n",
    "        x=torch.matmul(x,self.weight)+self.bias\n",
    "        return F.relu(x)\n",
    "linear=MyLinear(5,3)\n",
    "print(linear.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706f3a3",
   "metadata": {},
   "source": [
    "**5.5  读写文件**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb8b99bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n",
      "2\n",
      "cpu\n",
      "x1.device= cuda:0\n",
      "x2.device= cuda:1\n",
      "net.device= cpu\n"
     ]
    }
   ],
   "source": [
    "import torch,os\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden=MyLinear(20,256)\n",
    "        self.out=MyLinear(256,10)\n",
    "    def forward(self,x):\n",
    "        return self.out(self.hidden(x))\n",
    "\n",
    "\n",
    "net=MLP()\n",
    "x=torch.randn(2,20)\n",
    "y=net(x)\n",
    "path=r\"/home/pumengyu/2025_9python/limu/d2l\"\n",
    "torch.save(net.state_dict(),os.path.join(path,'mlp.params'))\n",
    "\n",
    "clone=MLP()\n",
    "clone.load_state_dict(torch.load(os.path.join(path,'mlp.params')))\n",
    "clone.eval()\n",
    "y_clone=clone(x)\n",
    "print(y_clone==y)\n",
    "print(torch.cuda.device_count())\n",
    "torch.cuda.device_count()\n",
    "x=torch.randn(2,20)\n",
    "print(x.device)\n",
    "def try_gpu(i=0):\n",
    "    if torch.cuda.device_count()>=i+1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "x1=torch.ones(2,3,device=try_gpu())\n",
    "print(\"x1.device=\",x1.device)\n",
    "x2=torch.randn(2,3,device=try_gpu(1))\n",
    "print(\"x2.device=\",x2.device)\n",
    "net=nn.Sequential(nn.Linear(3,1))\n",
    "print(\"net.device=\",net[0].weight.data.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
